{"cells":[{"cell_type":"markdown","metadata":{"id":"rLtTfHZ1tbA7"},"source":["## Data Understanding"]},{"cell_type":"markdown","metadata":{"id":"zm1jdIU15LIM"},"source":["### Exploratory Data Analysis"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":559,"status":"ok","timestamp":1661351281897,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"},"user_tz":-180},"id":"hNqWEgzRgoSj"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import gc\n","import pickle"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["#read the raw data\n","raw_df = pd.read_pickle('../data/telecom_xdr.pkl')\n","#drop any duplicate rows\n","raw_df_droped_dup = raw_df.drop_duplicates()\n","#associeted column descriptions\n","colum_descript = pd.read_excel('../resources/Field Descriptions.xlsx')"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Bearer Id</th>\n","      <th>Start</th>\n","      <th>Start ms</th>\n","      <th>End</th>\n","      <th>End ms</th>\n","      <th>Dur. (ms)</th>\n","      <th>IMSI</th>\n","      <th>MSISDN/Number</th>\n","      <th>IMEI</th>\n","      <th>Last Location Name</th>\n","      <th>...</th>\n","      <th>Youtube DL (Bytes)</th>\n","      <th>Youtube UL (Bytes)</th>\n","      <th>Netflix DL (Bytes)</th>\n","      <th>Netflix UL (Bytes)</th>\n","      <th>Gaming DL (Bytes)</th>\n","      <th>Gaming UL (Bytes)</th>\n","      <th>Other DL (Bytes)</th>\n","      <th>Other UL (Bytes)</th>\n","      <th>Total UL (Bytes)</th>\n","      <th>Total DL (Bytes)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.311448e+19</td>\n","      <td>4/4/2019 12:01</td>\n","      <td>770.0</td>\n","      <td>4/25/2019 14:35</td>\n","      <td>662.0</td>\n","      <td>1823652.0</td>\n","      <td>2.082014e+14</td>\n","      <td>3.366496e+10</td>\n","      <td>3.552121e+13</td>\n","      <td>9.16456699548519E+015</td>\n","      <td>...</td>\n","      <td>15854611.0</td>\n","      <td>2501332.0</td>\n","      <td>8198936.0</td>\n","      <td>9656251.0</td>\n","      <td>278082303.0</td>\n","      <td>14344150.0</td>\n","      <td>171744450.0</td>\n","      <td>8814393.0</td>\n","      <td>36749741.0</td>\n","      <td>308879636.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.311448e+19</td>\n","      <td>4/9/2019 13:04</td>\n","      <td>235.0</td>\n","      <td>4/25/2019 8:15</td>\n","      <td>606.0</td>\n","      <td>1365104.0</td>\n","      <td>2.082019e+14</td>\n","      <td>3.368185e+10</td>\n","      <td>3.579401e+13</td>\n","      <td>L77566A</td>\n","      <td>...</td>\n","      <td>20247395.0</td>\n","      <td>19111729.0</td>\n","      <td>18338413.0</td>\n","      <td>17227132.0</td>\n","      <td>608750074.0</td>\n","      <td>1170709.0</td>\n","      <td>526904238.0</td>\n","      <td>15055145.0</td>\n","      <td>53800391.0</td>\n","      <td>653384965.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.311448e+19</td>\n","      <td>4/9/2019 17:42</td>\n","      <td>1.0</td>\n","      <td>4/25/2019 11:58</td>\n","      <td>652.0</td>\n","      <td>1361762.0</td>\n","      <td>2.082003e+14</td>\n","      <td>3.376063e+10</td>\n","      <td>3.528151e+13</td>\n","      <td>D42335A</td>\n","      <td>...</td>\n","      <td>19725661.0</td>\n","      <td>14699576.0</td>\n","      <td>17587794.0</td>\n","      <td>6163408.0</td>\n","      <td>229584621.0</td>\n","      <td>395630.0</td>\n","      <td>410692588.0</td>\n","      <td>4215763.0</td>\n","      <td>27883638.0</td>\n","      <td>279807335.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.311448e+19</td>\n","      <td>4/10/2019 0:31</td>\n","      <td>486.0</td>\n","      <td>4/25/2019 7:36</td>\n","      <td>171.0</td>\n","      <td>1321509.0</td>\n","      <td>2.082014e+14</td>\n","      <td>3.375034e+10</td>\n","      <td>3.535661e+13</td>\n","      <td>T21824A</td>\n","      <td>...</td>\n","      <td>21388122.0</td>\n","      <td>15146643.0</td>\n","      <td>13994646.0</td>\n","      <td>1097942.0</td>\n","      <td>799538153.0</td>\n","      <td>10849722.0</td>\n","      <td>749039933.0</td>\n","      <td>12797283.0</td>\n","      <td>43324218.0</td>\n","      <td>846028530.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.311448e+19</td>\n","      <td>4/12/2019 20:10</td>\n","      <td>565.0</td>\n","      <td>4/25/2019 10:40</td>\n","      <td>954.0</td>\n","      <td>1089009.0</td>\n","      <td>2.082014e+14</td>\n","      <td>3.369980e+10</td>\n","      <td>3.540701e+13</td>\n","      <td>D88865A</td>\n","      <td>...</td>\n","      <td>15259380.0</td>\n","      <td>18962873.0</td>\n","      <td>17124581.0</td>\n","      <td>415218.0</td>\n","      <td>527707248.0</td>\n","      <td>3529801.0</td>\n","      <td>550709500.0</td>\n","      <td>13910322.0</td>\n","      <td>38542814.0</td>\n","      <td>569138589.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>1.311448e+19</td>\n","      <td>4/22/2019 7:38</td>\n","      <td>804.0</td>\n","      <td>4/25/2019 0:01</td>\n","      <td>199.0</td>\n","      <td>231780.0</td>\n","      <td>2.082017e+14</td>\n","      <td>3.361513e+10</td>\n","      <td>3.520031e+13</td>\n","      <td>L73684B</td>\n","      <td>...</td>\n","      <td>7438887.0</td>\n","      <td>13154981.0</td>\n","      <td>753238.0</td>\n","      <td>9887128.0</td>\n","      <td>124180544.0</td>\n","      <td>14967821.0</td>\n","      <td>350039548.0</td>\n","      <td>14932623.0</td>\n","      <td>53052136.0</td>\n","      <td>139043208.0</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>1.311448e+19</td>\n","      <td>4/22/2019 7:54</td>\n","      <td>676.0</td>\n","      <td>4/25/2019 0:01</td>\n","      <td>176.0</td>\n","      <td>230839.0</td>\n","      <td>2.082014e+14</td>\n","      <td>3.366495e+10</td>\n","      <td>8.636750e+13</td>\n","      <td>D73608B</td>\n","      <td>...</td>\n","      <td>3334865.0</td>\n","      <td>14454366.0</td>\n","      <td>17917894.0</td>\n","      <td>1530332.0</td>\n","      <td>293492651.0</td>\n","      <td>1362536.0</td>\n","      <td>495376623.0</td>\n","      <td>11684759.0</td>\n","      <td>30713366.0</td>\n","      <td>329159454.0</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>7.349883e+18</td>\n","      <td>4/22/2019 8:00</td>\n","      <td>810.0</td>\n","      <td>4/25/2019 2:22</td>\n","      <td>294.0</td>\n","      <td>238915.0</td>\n","      <td>2.082010e+14</td>\n","      <td>3.366403e+10</td>\n","      <td>3.586111e+13</td>\n","      <td>D73605A</td>\n","      <td>...</td>\n","      <td>23039020.0</td>\n","      <td>21457678.0</td>\n","      <td>1114842.0</td>\n","      <td>13958301.0</td>\n","      <td>516508263.0</td>\n","      <td>8432981.0</td>\n","      <td>624397048.0</td>\n","      <td>9207067.0</td>\n","      <td>57680454.0</td>\n","      <td>546747900.0</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>1.311448e+19</td>\n","      <td>4/22/2019 8:15</td>\n","      <td>473.0</td>\n","      <td>4/25/2019 2:55</td>\n","      <td>731.0</td>\n","      <td>239954.0</td>\n","      <td>2.082010e+14</td>\n","      <td>3.366875e+10</td>\n","      <td>3.533251e+13</td>\n","      <td>L76864A</td>\n","      <td>...</td>\n","      <td>14851113.0</td>\n","      <td>1312259.0</td>\n","      <td>11225027.0</td>\n","      <td>15097330.0</td>\n","      <td>522050751.0</td>\n","      <td>1138947.0</td>\n","      <td>281699270.0</td>\n","      <td>3082007.0</td>\n","      <td>21749419.0</td>\n","      <td>549470626.0</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>7.349883e+18</td>\n","      <td>4/22/2019 8:19</td>\n","      <td>899.0</td>\n","      <td>4/25/2019 3:17</td>\n","      <td>135.0</td>\n","      <td>241108.0</td>\n","      <td>2.082099e+14</td>\n","      <td>3.376173e+10</td>\n","      <td>3.572851e+13</td>\n","      <td>T18347B</td>\n","      <td>...</td>\n","      <td>22908155.0</td>\n","      <td>5664147.0</td>\n","      <td>5222307.0</td>\n","      <td>10713182.0</td>\n","      <td>720427913.0</td>\n","      <td>2522820.0</td>\n","      <td>570036174.0</td>\n","      <td>2081572.0</td>\n","      <td>25383544.0</td>\n","      <td>752194916.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 55 columns</p>\n","</div>"],"text/plain":["       Bearer Id            Start  Start ms              End  End ms  \\\n","0   1.311448e+19   4/4/2019 12:01     770.0  4/25/2019 14:35   662.0   \n","1   1.311448e+19   4/9/2019 13:04     235.0   4/25/2019 8:15   606.0   \n","2   1.311448e+19   4/9/2019 17:42       1.0  4/25/2019 11:58   652.0   \n","3   1.311448e+19   4/10/2019 0:31     486.0   4/25/2019 7:36   171.0   \n","4   1.311448e+19  4/12/2019 20:10     565.0  4/25/2019 10:40   954.0   \n","..           ...              ...       ...              ...     ...   \n","95  1.311448e+19   4/22/2019 7:38     804.0   4/25/2019 0:01   199.0   \n","96  1.311448e+19   4/22/2019 7:54     676.0   4/25/2019 0:01   176.0   \n","97  7.349883e+18   4/22/2019 8:00     810.0   4/25/2019 2:22   294.0   \n","98  1.311448e+19   4/22/2019 8:15     473.0   4/25/2019 2:55   731.0   \n","99  7.349883e+18   4/22/2019 8:19     899.0   4/25/2019 3:17   135.0   \n","\n","    Dur. (ms)          IMSI  MSISDN/Number          IMEI  \\\n","0   1823652.0  2.082014e+14   3.366496e+10  3.552121e+13   \n","1   1365104.0  2.082019e+14   3.368185e+10  3.579401e+13   \n","2   1361762.0  2.082003e+14   3.376063e+10  3.528151e+13   \n","3   1321509.0  2.082014e+14   3.375034e+10  3.535661e+13   \n","4   1089009.0  2.082014e+14   3.369980e+10  3.540701e+13   \n","..        ...           ...            ...           ...   \n","95   231780.0  2.082017e+14   3.361513e+10  3.520031e+13   \n","96   230839.0  2.082014e+14   3.366495e+10  8.636750e+13   \n","97   238915.0  2.082010e+14   3.366403e+10  3.586111e+13   \n","98   239954.0  2.082010e+14   3.366875e+10  3.533251e+13   \n","99   241108.0  2.082099e+14   3.376173e+10  3.572851e+13   \n","\n","       Last Location Name  ...  Youtube DL (Bytes)  Youtube UL (Bytes)  \\\n","0   9.16456699548519E+015  ...          15854611.0           2501332.0   \n","1                 L77566A  ...          20247395.0          19111729.0   \n","2                 D42335A  ...          19725661.0          14699576.0   \n","3                 T21824A  ...          21388122.0          15146643.0   \n","4                 D88865A  ...          15259380.0          18962873.0   \n","..                    ...  ...                 ...                 ...   \n","95                L73684B  ...           7438887.0          13154981.0   \n","96                D73608B  ...           3334865.0          14454366.0   \n","97                D73605A  ...          23039020.0          21457678.0   \n","98                L76864A  ...          14851113.0           1312259.0   \n","99                T18347B  ...          22908155.0           5664147.0   \n","\n","    Netflix DL (Bytes)  Netflix UL (Bytes)  Gaming DL (Bytes)  \\\n","0            8198936.0           9656251.0        278082303.0   \n","1           18338413.0          17227132.0        608750074.0   \n","2           17587794.0           6163408.0        229584621.0   \n","3           13994646.0           1097942.0        799538153.0   \n","4           17124581.0            415218.0        527707248.0   \n","..                 ...                 ...                ...   \n","95            753238.0           9887128.0        124180544.0   \n","96          17917894.0           1530332.0        293492651.0   \n","97           1114842.0          13958301.0        516508263.0   \n","98          11225027.0          15097330.0        522050751.0   \n","99           5222307.0          10713182.0        720427913.0   \n","\n","    Gaming UL (Bytes)  Other DL (Bytes)  Other UL (Bytes)  Total UL (Bytes)  \\\n","0          14344150.0       171744450.0         8814393.0        36749741.0   \n","1           1170709.0       526904238.0        15055145.0        53800391.0   \n","2            395630.0       410692588.0         4215763.0        27883638.0   \n","3          10849722.0       749039933.0        12797283.0        43324218.0   \n","4           3529801.0       550709500.0        13910322.0        38542814.0   \n","..                ...               ...               ...               ...   \n","95         14967821.0       350039548.0        14932623.0        53052136.0   \n","96          1362536.0       495376623.0        11684759.0        30713366.0   \n","97          8432981.0       624397048.0         9207067.0        57680454.0   \n","98          1138947.0       281699270.0         3082007.0        21749419.0   \n","99          2522820.0       570036174.0         2081572.0        25383544.0   \n","\n","    Total DL (Bytes)  \n","0        308879636.0  \n","1        653384965.0  \n","2        279807335.0  \n","3        846028530.0  \n","4        569138589.0  \n","..               ...  \n","95       139043208.0  \n","96       329159454.0  \n","97       546747900.0  \n","98       549470626.0  \n","99       752194916.0  \n","\n","[100 rows x 55 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["raw_df.head(100)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":362,"status":"ok","timestamp":1661342355630,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"},"user_tz":-180},"id":"DR5uCLgvgoSt","outputId":"9c9b078a-fa5b-4db6-8c9b-408bd9978ee5"},"outputs":[{"data":{"text/plain":["['bearer id',\n"," 'Dur. (s)',\n"," 'YouTube DL (Bytes)',\n"," 'YouTube UL (Bytes)',\n"," 'Other DL',\n"," 'Other UL']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# column names missmatch b/n data and description\n","[x for x in colum_descript['Fields'].tolist() if x not in raw_df.columns.tolist()]"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["['Bearer Id',\n"," 'Youtube DL (Bytes)',\n"," 'Youtube UL (Bytes)',\n"," 'Other DL (Bytes)',\n"," 'Other UL (Bytes)']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["[x for x in raw_df.columns.tolist() if x not in colum_descript['Fields'].tolist()]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["1303"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["#Match columns in the description df to the data df\n","raw_df_renamed = raw_df.rename(columns={'Bearer Id':'bearer id' , 'Youtube DL (Bytes)':'YouTube DL (Bytes)', 'Youtube UL (Bytes)':'YouTube UL (Bytes)', 'Other DL (Bytes)':'Other DL', 'Other UL (Bytes)':'Other UL'})\n","del raw_df\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"gyQ1k3lj5h2c"},"source":["### Data Pre-Processing\n","Data preprocessing is an integral step in Machine Learning as the quality of data and the useful information that can be derived from it directly affects the ability of our model to learn; therefore, it is extremely important that we preprocess our data before feeding it into our model.\n","#### Handling Null Values"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":397,"status":"ok","timestamp":1661342442100,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"},"user_tz":-180},"id":"cudEnVCrgoSt","outputId":"6c74fe6a-0174-4048-f632-6c468a93e573"},"outputs":[{"name":"stdout","output_type":"stream","text":["The Diabetes dataset contains 12.5 % missing values.\n"," 10 columns are missing moethan 30% of their values\n","They are:\n","\n","                                      Fields                                        Description  % Missing\n","15               TCP DL Retrans. Vol (Bytes)  TCP volume of Downlink packets detected as ret...  58.763608\n","16               TCP UL Retrans. Vol (Bytes)  TCP volume of Uplink packets detected as retra...  64.432237\n","25                           HTTP DL (Bytes)  HTTP data volume (in Bytes) received by the MS...  54.315638\n","26                           HTTP UL (Bytes)  HTTP data volume (in Bytes) sent by the MS dur...  54.539636\n","32           Nb of sec with 125000B < Vol DL              Number of seconds with IP Volume DL >  65.024900\n","33     Nb of sec with 1250B < Vol UL < 6250B      Number of seconds with IP Volume UL between …  61.928920\n","34  Nb of sec with 31250B < Vol DL < 125000B      Number of seconds with IP Volume DL between …  62.390251\n","35            Nb of sec with 37500B < Vol UL              Number of seconds with IP Volume UL >  86.835421\n","36    Nb of sec with 6250B < Vol DL < 31250B      Number of seconds with IP Volume DL between …  58.877607\n","37    Nb of sec with 6250B < Vol UL < 37500B      Number of seconds with IP Volume UL between …  74.561503\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_10750/2958242492.py:22: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  droped['% Missing'] = [colum_wise.get(col_nm) for col_nm in col_names]\n"]},{"data":{"text/plain":["0"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["# how many missing values exist or better still what is the % of missing values in the dataset?\n","def percent_missing(df):\n","\n","    # Calculate total number of cells in dataframe\n","    totalCells = np.prod(df.shape)\n","\n","    # Count number of missing values per column\n","    missingCount = df.isnull().sum()\n","\n","    # Calculate total number of missing values\n","    totalMissing = missingCount.sum()\n","\n","    # Calculate percentage of missing values\n","    print(\"The Diabetes dataset contains\", round(((totalMissing/totalCells) * 100), 2), \"%\", \"missing values.\")\n","\n","def columns_missing_most_values(df, decript, percentage):\n","    colum_wise = df.isnull().sum() * 100 /df.shape[0]\n","    col_names = colum_wise[colum_wise > percentage].index.tolist() \n","    print(\" {} columns are missing moethan {}% of their values\\nThey are:\\n\".format(len(col_names), percentage))\n","    with pd.option_context('expand_frame_repr', False):\n","        droped = decript.loc[decript['Fields'].isin(col_names)]\n","        droped['% Missing'] = [colum_wise.get(col_nm) for col_nm in col_names]\n","        print (droped)\n","    return col_names\n","\n","percent_missing(raw_df_renamed)\n","# drop columns with more than 30% missing values\n","to_drop = columns_missing_most_values(raw_df_renamed, colum_descript, 30)\n","raw_df_droped = raw_df_renamed.drop(to_drop, axis=1)\n","raw_df_droped.shape\n","del raw_df_renamed\n","gc.collect()\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":[" 31 columns are missing atleast 1 value, while 14 have no missing values.\n","The columns missing values are:\n","\n","                            Fields                                        Description  % Missing\n","55                Total UL (Bytes)  Data volume (in Bytes) sent by the MS during t...   0.000667\n","1                        Dur. (ms)                  Total Duration of the xDR (in ms)   0.000667\n","2                            Start      Start time of the xDR (first frame timestamp)   0.000667\n","3                         Start ms  Milliseconds offset of start time for the xDR ...   0.000667\n","4                              End         End time of the xDR (last frame timestamp)   0.000667\n","5                           End ms  Milliseconds offset of end time of the xDR (la...   0.000667\n","27       Activity Duration DL (ms)  Activity Duration for downlink (ms) - excludin...   0.000667\n","54                Total DL (Bytes)  Data volume (in Bytes) received by the MS duri...   0.000667\n","14         Avg Bearer TP UL (kbps)  Average Bearer Throughput for uplink (kbps) - ...   0.000667\n","29                     Dur. (ms).1                  Total Duration of the xDR (in ms)   0.000667\n","28       Activity Duration UL (ms)  Activity Duration for uplink (ms) - excluding ...   0.000667\n","13         Avg Bearer TP DL (kbps)  Average Bearer Throughput for Downlink (kbps) ...   0.000667\n","7                             IMSI           International Mobile Subscriber Identity   0.379997\n","9                             IMEI            International Mobile Equipment Identity   0.381331\n","31                    Handset Type                  Handset type of the mobile device   0.381331\n","30            Handset Manufacturer                              Handset manufacturer    0.381331\n","20              DL TP > 1 Mbps (%)  Duration ratio when Bearer Downlink Throughput...   0.502663\n","18  50 Kbps < DL TP < 250 Kbps (%)  Duration ratio when Bearer Downlink Throughput...   0.502663\n","17             DL TP < 50 Kbps (%)  Duration ratio when Bearer Downlink Throughput...   0.502663\n","19   250 Kbps < DL TP < 1 Mbps (%)  Duration ratio when Bearer Downlink Throughput...   0.502663\n","38   Nb of sec with Vol DL < 6250B              Number of seconds with IP Volume DL <   0.503330\n","21             UL TP < 10 Kbps (%)  Duration ratio when Bearer Uplink Throughput < ….   0.527996\n","22   10 Kbps < UL TP < 50 Kbps (%)  Duration ratio when Bearer Uplink Throughput r...   0.527996\n","23  50 Kbps < UL TP < 300 Kbps (%)  Duration ratio when Bearer Uplink Throughput r...   0.527996\n","24            UL TP > 300 Kbps (%)  Duration ratio when Bearer Uplink Throughput > ….   0.527996\n","39   Nb of sec with Vol UL < 1250B              Number of seconds with IP Volume UL <   0.528663\n","0                        bearer id                             xDr session identifier   0.660662\n","8                    MSISDN/Number  MS International PSTN/ISDN Number of mobile - ...   0.710662\n","10              Last Location Name  User location call name (2G/3G/4G) at the end ...   0.768662\n","12                 Avg RTT UL (ms)  Average Round Trip Time measurement Uplink dir...  18.541210\n","11                 Avg RTT DL (ms)  Average Round Trip Time measurement Downlink d...  18.552543\n","\n"," The columns with full data are:\n","\n","                     Fields                                        Description  % Missing\n","40  Social Media DL (Bytes)  Social Media data volume (in Bytes) received b...        0.0\n","41  Social Media UL (Bytes)  Social Media data volume (in Bytes) sent by th...        0.0\n","42       YouTube DL (Bytes)  YouTube data volume (in Bytes) received by the...        0.0\n","43       YouTube UL (Bytes)  YouTube data volume (in Bytes) sent by the MS ...        0.0\n","44       Netflix DL (Bytes)  Netflix data volume (in Bytes) received by the...        0.0\n","45       Netflix UL (Bytes)  Netflix data volume (in Bytes) sent by the MS ...        0.0\n","46        Google DL (Bytes)  Google data volume (in Bytes) Received by the ...        0.0\n","47        Google UL (Bytes)  Google data volume (in Bytes) sent by the MS d...        0.0\n","48         Email DL (Bytes)  Email data volume (in Bytes) Received by the M...        0.0\n","49         Email UL (Bytes)  Email data volume (in Bytes) sent by the MS du...        0.0\n","50        Gaming DL (Bytes)  Gaming data volume (in Bytes) Received by the ...        0.0\n","51        Gaming UL (Bytes)  Gaming data volume (in Bytes) sent by the MS d...        0.0\n","52                 Other DL  Other data volume (in Bytes) received by the M...        0.0\n","53                 Other UL  Other data volume (in Bytes) sent by the MS du...        0.0\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_10750/2102104408.py:8: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  with_missing['% Missing'] = [colum_wise.get(col_nm) for col_nm in missing_col_names]\n","/tmp/ipykernel_10750/2102104408.py:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  complete['% Missing'] = [colum_wise.get(col_nm) for col_nm in complete_col_names]\n"]}],"source":["def columns_missing_values(df, decript):\n","    colum_wise = df.isnull().sum() * 100 /df.shape[0]\n","    missing_col_names = colum_wise[colum_wise > 0].index.tolist()\n","    complete_col_names = [cn for cn in df.columns if cn not in missing_col_names]\n","    print(\" {} columns are missing atleast 1 value, while {} have no missing values.\\nThe columns missing values are:\\n\".format(len(missing_col_names), len(complete_col_names)))\n","    with pd.option_context('expand_frame_repr', False):\n","        with_missing = decript.loc[decript['Fields'].isin(missing_col_names)]\n","        with_missing['% Missing'] = [colum_wise.get(col_nm) for col_nm in missing_col_names]\n","        print (with_missing.sort_values(by=['% Missing']))\n","        print(\"\\n The columns with full data are:\\n\")\n","        complete = decript.loc[decript['Fields'].isin(complete_col_names)]\n","        complete['% Missing'] = [colum_wise.get(col_nm) for col_nm in complete_col_names]\n","        print (complete)\n","    return missing_col_names, complete_col_names\n","# inspect the rest of the missimng values(their description)\n","to_fill, complet = columns_missing_values(raw_df_droped, colum_descript)"]},{"cell_type":"markdown","metadata":{},"source":["##### The 45 data columns can be grouped in to three different data-types:\n","* Categorical:\n","\n","['Handset Type', 'Handset Manufacturer', 'Last Location Name', 'IMEI', 'MSISDN/Number', 'IMSI', 'bearer id']\n","* Time Series:\n","\n","['Start ms', 'Start', 'End ms', 'End']\n","* Numerical contin:\n","\n","['DL TP < 50 Kbps (%)', '50 Kbps < DL TP < 250 Kbps (%)', '250 Kbps < DL TP < 1 Mbps (%)', 'DL TP > 1 Mbps (%)', 'UL TP < 10 Kbps (%)', '10 Kbps < UL TP < 50 Kbps (%)', '50 Kbps < UL TP < 300 Kbps (%)', 'UL TP > 300 Kbps (%)']\n","* Numerical descrite:\n","\n","['Dur. (ms)', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)', 'DL TP < 50 Kbps (%)',\n"," '50 Kbps < DL TP < 250 Kbps (%)', '250 Kbps < DL TP < 1 Mbps (%)', 'DL TP > 1 Mbps (%)', 'UL TP < 10 Kbps (%)',\n"," '10 Kbps < UL TP < 50 Kbps (%)', '50 Kbps < UL TP < 300 Kbps (%)', 'UL TP > 300 Kbps (%)', 'Activity Duration DL (ms)',\n"," 'Activity Duration UL (ms)', 'Dur. (ms).1', 'Nb of sec with Vol DL < 6250B', 'Nb of sec with Vol UL < 1250B',\n"," 'Social Media DL (Bytes)', 'Social Media UL (Bytes)', 'Google DL (Bytes)', 'Google UL (Bytes)', 'Email DL (Bytes)',\n"," 'Email UL (Bytes)', 'YouTube DL (Bytes)', 'YouTube UL (Bytes)', 'Netflix DL (Bytes)', 'Netflix UL (Bytes)', 'Gaming DL (Bytes)',\n"," 'Gaming UL (Bytes)', 'Other DL', 'Other UL', 'Total UL (Bytes)', 'Total DL (Bytes)']"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["categorical_columns = ['Handset Type', 'Handset Manufacturer', 'Last Location Name', 'IMEI', 'MSISDN/Number', 'IMSI', 'bearer id']\n","time_series_columns = ['Start ms', 'Start', 'End ms', 'End']\n","numer_cont_colmns = ['DL TP < 50 Kbps (%)', '50 Kbps < DL TP < 250 Kbps (%)', '250 Kbps < DL TP < 1 Mbps (%)', 'DL TP > 1 Mbps (%)', \n","                    'UL TP < 10 Kbps (%)', '10 Kbps < UL TP < 50 Kbps (%)', '50 Kbps < UL TP < 300 Kbps (%)', 'UL TP > 300 Kbps (%)']\n","numer_discr_colmns = ['Dur. (ms)', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)', \n","                      'DL TP < 50 Kbps (%)', '50 Kbps < DL TP < 250 Kbps (%)', '250 Kbps < DL TP < 1 Mbps (%)', 'DL TP > 1 Mbps (%)', \n","                      'UL TP < 10 Kbps (%)','10 Kbps < UL TP < 50 Kbps (%)', '50 Kbps < UL TP < 300 Kbps (%)', 'UL TP > 300 Kbps (%)', \n","                      'Activity Duration DL (ms)','Activity Duration UL (ms)', 'Dur. (ms).1', 'Nb of sec with Vol DL < 6250B',\n","                      'Nb of sec with Vol UL < 1250B','Social Media DL (Bytes)', 'Social Media UL (Bytes)', 'Google DL (Bytes)', \n","                      'Google UL (Bytes)', 'Email DL (Bytes)','Email UL (Bytes)', 'YouTube DL (Bytes)', 'YouTube UL (Bytes)', 'Netflix DL (Bytes)', 'Netflix UL (Bytes)', 'Gaming DL (Bytes)',\n","                      'Gaming UL (Bytes)', 'Other DL', 'Other UL', 'Total UL (Bytes)', 'Total DL (Bytes)']"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["No of unique values in the columns missing values\n","                            Fields                                        Description  uniques\n","24            UL TP > 300 Kbps (%)  Duration ratio when Bearer Uplink Throughput > ….       63\n","23  50 Kbps < UL TP < 300 Kbps (%)  Duration ratio when Bearer Uplink Throughput r...       68\n","19   250 Kbps < DL TP < 1 Mbps (%)  Duration ratio when Bearer Downlink Throughput...       73\n","18  50 Kbps < DL TP < 250 Kbps (%)  Duration ratio when Bearer Downlink Throughput...       84\n","22   10 Kbps < UL TP < 50 Kbps (%)  Duration ratio when Bearer Uplink Throughput r...       85\n","20              DL TP > 1 Mbps (%)  Duration ratio when Bearer Downlink Throughput...       85\n","21             UL TP < 10 Kbps (%)  Duration ratio when Bearer Uplink Throughput < ….       98\n","17             DL TP < 50 Kbps (%)  Duration ratio when Bearer Downlink Throughput...      100\n","30            Handset Manufacturer                              Handset manufacturer       170\n","12                 Avg RTT UL (ms)  Average Round Trip Time measurement Uplink dir...      722\n","2                            Start      Start time of the xDR (first frame timestamp)     1000\n","4                              End         End time of the xDR (last frame timestamp)     1000\n","31                    Handset Type                  Handset type of the mobile device     1396\n","11                 Avg RTT DL (ms)  Average Round Trip Time measurement Downlink d...     2130\n","3                         Start ms  Milliseconds offset of start time for the xDR ...     6403\n","1                        Dur. (ms)                  Total Duration of the xDR (in ms)     9997\n","14         Avg Bearer TP UL (kbps)  Average Bearer Throughput for uplink (kbps) - ...    14528\n","38   Nb of sec with Vol DL < 6250B              Number of seconds with IP Volume DL <    22270\n","39   Nb of sec with Vol UL < 1250B              Number of seconds with IP Volume UL <    22976\n","13         Avg Bearer TP DL (kbps)  Average Bearer Throughput for Downlink (kbps) ...    41753\n","10              Last Location Name  User location call name (2G/3G/4G) at the end ...    45547\n","5                           End ms  Milliseconds offset of end time of the xDR (la...    89525\n","27       Activity Duration DL (ms)  Activity Duration for downlink (ms) - excludin...   102560\n","28       Activity Duration UL (ms)  Activity Duration for uplink (ms) - excluding ...   106292\n","8                    MSISDN/Number  MS International PSTN/ISDN Number of mobile - ...   106856\n","7                             IMSI           International Mobile Subscriber Identity   107265\n","9                             IMEI            International Mobile Equipment Identity   107270\n","29                     Dur. (ms).1                  Total Duration of the xDR (in ms)   122871\n","0                        bearer id                             xDr session identifier   134708\n","54                Total DL (Bytes)  Data volume (in Bytes) received by the MS duri...   149728\n","55                Total UL (Bytes)  Data volume (in Bytes) sent by the MS during t...   149987\n","\n","No of unique values in the columns without any missing values\n","                     Fields                                        Description  uniques\n","41  Social Media UL (Bytes)  Social Media data volume (in Bytes) sent by th...    59078\n","45       Netflix UL (Bytes)  Netflix data volume (in Bytes) sent by the MS ...   138699\n","40  Social Media DL (Bytes)  Social Media data volume (in Bytes) received b...   146856\n","44       Netflix DL (Bytes)  Netflix data volume (in Bytes) received by the...   146916\n","43       YouTube UL (Bytes)  YouTube data volume (in Bytes) sent by the MS ...   147267\n","42       YouTube DL (Bytes)  YouTube data volume (in Bytes) received by the...   149024\n","53                 Other UL  Other data volume (in Bytes) sent by the MS du...   149284\n","51        Gaming UL (Bytes)  Gaming data volume (in Bytes) sent by the MS d...   149316\n","47        Google UL (Bytes)  Google data volume (in Bytes) sent by the MS d...   149477\n","49         Email UL (Bytes)  Email data volume (in Bytes) sent by the MS du...   149490\n","48         Email DL (Bytes)  Email data volume (in Bytes) Received by the M...   149518\n","46        Google DL (Bytes)  Google data volume (in Bytes) Received by the ...   149547\n","50        Gaming DL (Bytes)  Gaming data volume (in Bytes) Received by the ...   149983\n","52                 Other DL  Other data volume (in Bytes) received by the M...   149986\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_10750/1916892460.py:6: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  uniques['uniques'] = [col_wise_unique.get(col_nm) for col_nm in col_names]\n","/tmp/ipykernel_10750/1916892460.py:6: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  uniques['uniques'] = [col_wise_unique.get(col_nm) for col_nm in col_names]\n"]}],"source":["#print sorted no_of_unique values per column \n","def columns_uniqueness(df, decript):\n","    col_wise_unique = df.nunique(dropna=True)\n","    col_names = col_wise_unique.index.tolist()\n","    uniques = decript.loc[decript['Fields'].isin(col_names)]\n","    uniques['uniques'] = [col_wise_unique.get(col_nm) for col_nm in col_names]\n","    uniques.sort_values(by=['uniques'])\n","    with pd.option_context('expand_frame_repr', False):\n","        print (uniques.sort_values(by=['uniques']))\n","\n","def print_sorted_no_uniques(df, decript):\n","    #for the columsn with the missing values\n","    print(\"No of unique values in the columns missing values\")\n","    df_missing_values = df.drop(complet, axis=1)\n","    columns_uniqueness(df_missing_values, decript)\n","\n","    #for the columns without any missing values\n","    print(\"\\nNo of unique values in the columns without any missing values\")\n","    df_complete_values = df.drop(to_fill, axis=1)\n","    columns_uniqueness(df_complete_values, decript)\n","\n","print_sorted_no_uniques(raw_df_droped, colum_descript)"]},{"cell_type":"markdown","metadata":{},"source":["##### !! The is no single unique column to be used as key"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# Fill all the 8 contineous numerical columns(% ratios) with 0.0 as it is a probaility dist.(needs to addup to one)\n","values_0 = { k:v for (k,v) in zip(numer_cont_colmns, [0.0] * 8)} \n","raw_df_droped_fill_0 = raw_df_droped.fillna(value=values_0)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Fill the 3 string categorical columns with 'undefined'\n","values_1 = { k:v for (k,v) in zip(['Handset Type', 'Handset Manufacturer', 'Last Location Name'], ['undefined'] * 3)} \n","raw_df_droped_fill_1 = raw_df_droped_fill_0.fillna(value=values_1)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Fill the rest of the categorical columns with 0\n","values_2 = { k:v for (k,v) in zip(['IMEI', 'MSISDN/Number', 'IMSI', 'bearer id'],[0]*3)}\n","raw_df_droped_fill_2 = raw_df_droped_fill_1.fillna(value=values_2)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Fill the decrit numrical columns with 0, as they represent either a data-size in Bytes or time in Sec/mili Sec\n","values_3 = { k:v for (k,v) in zip(numer_discr_colmns,[0]*len(numer_discr_colmns))}\n","raw_df_droped_fill_3 = raw_df_droped_fill_2.fillna(value=values_3)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_10750/1122042077.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n","  raw_df_droped_fill_4 = raw_df_droped_fill_3.fillna( method='ffill')\n"]},{"name":"stdout","output_type":"stream","text":[" 0 columns are missing atleast 1 value, while 45 have no missing values.\n","The columns missing values are:\n","\n","Empty DataFrame\n","Columns: [Fields, Description, % Missing]\n","Index: []\n","\n"," The columns with full data are:\n","\n","                            Fields                                        Description  % Missing\n","0                        bearer id                             xDr session identifier        0.0\n","1                        Dur. (ms)                  Total Duration of the xDR (in ms)        0.0\n","2                            Start      Start time of the xDR (first frame timestamp)        0.0\n","3                         Start ms  Milliseconds offset of start time for the xDR ...        0.0\n","4                              End         End time of the xDR (last frame timestamp)        0.0\n","5                           End ms  Milliseconds offset of end time of the xDR (la...        0.0\n","7                             IMSI           International Mobile Subscriber Identity        0.0\n","8                    MSISDN/Number  MS International PSTN/ISDN Number of mobile - ...        0.0\n","9                             IMEI            International Mobile Equipment Identity        0.0\n","10              Last Location Name  User location call name (2G/3G/4G) at the end ...        0.0\n","11                 Avg RTT DL (ms)  Average Round Trip Time measurement Downlink d...        0.0\n","12                 Avg RTT UL (ms)  Average Round Trip Time measurement Uplink dir...        0.0\n","13         Avg Bearer TP DL (kbps)  Average Bearer Throughput for Downlink (kbps) ...        0.0\n","14         Avg Bearer TP UL (kbps)  Average Bearer Throughput for uplink (kbps) - ...        0.0\n","17             DL TP < 50 Kbps (%)  Duration ratio when Bearer Downlink Throughput...        0.0\n","18  50 Kbps < DL TP < 250 Kbps (%)  Duration ratio when Bearer Downlink Throughput...        0.0\n","19   250 Kbps < DL TP < 1 Mbps (%)  Duration ratio when Bearer Downlink Throughput...        0.0\n","20              DL TP > 1 Mbps (%)  Duration ratio when Bearer Downlink Throughput...        0.0\n","21             UL TP < 10 Kbps (%)  Duration ratio when Bearer Uplink Throughput < ….        0.0\n","22   10 Kbps < UL TP < 50 Kbps (%)  Duration ratio when Bearer Uplink Throughput r...        0.0\n","23  50 Kbps < UL TP < 300 Kbps (%)  Duration ratio when Bearer Uplink Throughput r...        0.0\n","24            UL TP > 300 Kbps (%)  Duration ratio when Bearer Uplink Throughput > ….        0.0\n","27       Activity Duration DL (ms)  Activity Duration for downlink (ms) - excludin...        0.0\n","28       Activity Duration UL (ms)  Activity Duration for uplink (ms) - excluding ...        0.0\n","29                     Dur. (ms).1                  Total Duration of the xDR (in ms)        0.0\n","30            Handset Manufacturer                              Handset manufacturer         0.0\n","31                    Handset Type                  Handset type of the mobile device        0.0\n","38   Nb of sec with Vol DL < 6250B              Number of seconds with IP Volume DL <        0.0\n","39   Nb of sec with Vol UL < 1250B              Number of seconds with IP Volume UL <        0.0\n","40         Social Media DL (Bytes)  Social Media data volume (in Bytes) received b...        0.0\n","41         Social Media UL (Bytes)  Social Media data volume (in Bytes) sent by th...        0.0\n","42              YouTube DL (Bytes)  YouTube data volume (in Bytes) received by the...        0.0\n","43              YouTube UL (Bytes)  YouTube data volume (in Bytes) sent by the MS ...        0.0\n","44              Netflix DL (Bytes)  Netflix data volume (in Bytes) received by the...        0.0\n","45              Netflix UL (Bytes)  Netflix data volume (in Bytes) sent by the MS ...        0.0\n","46               Google DL (Bytes)  Google data volume (in Bytes) Received by the ...        0.0\n","47               Google UL (Bytes)  Google data volume (in Bytes) sent by the MS d...        0.0\n","48                Email DL (Bytes)  Email data volume (in Bytes) Received by the M...        0.0\n","49                Email UL (Bytes)  Email data volume (in Bytes) sent by the MS du...        0.0\n","50               Gaming DL (Bytes)  Gaming data volume (in Bytes) Received by the ...        0.0\n","51               Gaming UL (Bytes)  Gaming data volume (in Bytes) sent by the MS d...        0.0\n","52                        Other DL  Other data volume (in Bytes) received by the M...        0.0\n","53                        Other UL  Other data volume (in Bytes) sent by the MS du...        0.0\n","54                Total DL (Bytes)  Data volume (in Bytes) received by the MS duri...        0.0\n","55                Total UL (Bytes)  Data volume (in Bytes) sent by the MS during t...        0.0\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_10750/2102104408.py:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  complete['% Missing'] = [colum_wise.get(col_nm) for col_nm in complete_col_names]\n"]},{"data":{"text/plain":["0"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# Fill the Time series(what are left) columns formawrd\n","raw_df_droped_fill_4 = raw_df_droped_fill_3.fillna( method='ffill')\n","raw_df_droped_fill_4.to_pickle(\"../data/df_cleaned.pkl\")\n","colum_descript.to_pickle(\"../data/colum_descript.pkl\")\n","with open('../data/col_types.pickle', 'wb') as handle:\n","    pickle.dump({'categorigal':categorical_columns, 'continuous_numerical':numer_cont_colmns, 'discrete_numerical':numer_discr_colmns, 'time_series':time_series_columns}, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","# To check for missing values\n","to_fill, complet = columns_missing_values(raw_df_droped_fill_4, colum_descript)\n","del categorical_columns, numer_cont_colmns, numer_discr_colmns, time_series_columns, raw_df_droped_fill_4, raw_df_droped_fill_3, raw_df_droped_fill_2, raw_df_droped_fill_1, raw_df_droped_fill_0, values_0, values_1, values_2, values_3\n","gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["##### Standardize the numerical columns with Z-score to get mean of 0 and sd of 1"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1130,"status":"ok","timestamp":1661348302659,"user":{"displayName":"Anastasia Kiiru","userId":"12437046474838202979"},"user_tz":-180},"id":"IyVptlldEuYg"},"outputs":[],"source":["'''df_standard = raw_df_droped_fill_4\n","for col in numer_dicrt_colmns + numer_cont_colmns:\n","    df_standard[col] = (raw_df_droped_fill_4[col] - raw_df_droped_fill_4[col].mean())/raw_df_droped_fill_4[col].std(ddof=0)\n","df_standard.to_pickle(\"../data/df_cleaned.pkl\")\n","colum_descript.to_pickle(\"../data/colum_descript.pkl\")\n","del raw_df_droped_fill_4, df_standard, colum_descript\n","gc.collect()'''"]},{"cell_type":"markdown","metadata":{},"source":["##### One-Hot-Encode the categoricals"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["#It is very resource(compute) intensive\n","#df_ohe = pd.get_dummies(df_standard, columns = categorical_columns[:3])"]},{"cell_type":"markdown","metadata":{},"source":["### EDA\n","#### Per user agrigates"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["df_cleaned = pd.read_pickle(\"../data/df_cleaned.pkl\")\n","df_cleaned['index'] = df_cleaned.index\n","col_descript = pd.read_pickle(\"../data/colum_descript.pkl\")\n","with open('../data/col_types.pickle', 'rb') as handle:\n","    col_types = pickle.load(handle)\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["Index(['bearer id', 'Start', 'Start ms', 'End', 'End ms', 'Dur. (ms)', 'IMSI',\n","       'MSISDN/Number', 'IMEI', 'Last Location Name', 'Avg RTT DL (ms)',\n","       'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',\n","       'DL TP < 50 Kbps (%)', '50 Kbps < DL TP < 250 Kbps (%)',\n","       '250 Kbps < DL TP < 1 Mbps (%)', 'DL TP > 1 Mbps (%)',\n","       'UL TP < 10 Kbps (%)', '10 Kbps < UL TP < 50 Kbps (%)',\n","       '50 Kbps < UL TP < 300 Kbps (%)', 'UL TP > 300 Kbps (%)',\n","       'Activity Duration DL (ms)', 'Activity Duration UL (ms)', 'Dur. (ms).1',\n","       'Handset Manufacturer', 'Handset Type', 'Nb of sec with Vol DL < 6250B',\n","       'Nb of sec with Vol UL < 1250B', 'Social Media DL (Bytes)',\n","       'Social Media UL (Bytes)', 'Google DL (Bytes)', 'Google UL (Bytes)',\n","       'Email DL (Bytes)', 'Email UL (Bytes)', 'YouTube DL (Bytes)',\n","       'YouTube UL (Bytes)', 'Netflix DL (Bytes)', 'Netflix UL (Bytes)',\n","       'Gaming DL (Bytes)', 'Gaming UL (Bytes)', 'Other DL', 'Other UL',\n","       'Total UL (Bytes)', 'Total DL (Bytes)', 'index'],\n","      dtype='object')"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["df_cleaned.columns"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_10750/70974939.py:1: FutureWarning: The provided callable <function sum at 0x7f3f91f26340> is currently using SeriesGroupBy.sum. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"sum\" instead.\n","  df_user_aggrigate = df_cleaned.groupby('MSISDN/Number').agg(\n"]}],"source":["df_user_aggrigate = df_cleaned.groupby('MSISDN/Number').agg(\n","    Total_DL=('Total DL (Bytes)', np.sum),Total_UL=('Total UL (Bytes)', np.sum),Duration_ms=('Dur. (ms).1', np.sum),\n","    Google_DL=('Google DL (Bytes)', np.sum),Google_UL=('Google UL (Bytes)', np.sum),Netflix_DL=('Netflix DL (Bytes)', np.sum),\n","    Netflix_UL=('Netflix UL (Bytes)', np.sum),Email_DL=('Email DL (Bytes)', np.sum),Email_UL=('Email UL (Bytes)', np.sum),\n","    YouTube_DL=('YouTube DL (Bytes)', np.sum),YouTube_UL=('YouTube UL (Bytes)', np.sum),Social_Media_DL=('Social Media DL (Bytes)', np.sum),\n","    Social_Media_UL=('Social Media UL (Bytes)', np.sum),Gaming_DL=('Gaming DL (Bytes)', np.sum),Gaming_UL=('Gaming UL (Bytes)', np.sum),\n","    Other_UL=('Other UL', np.sum),Other_DL=('Other DL', np.sum),\n","                                        )"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>bearer id</th>\n","      <th>Start</th>\n","      <th>Start ms</th>\n","      <th>End</th>\n","      <th>End ms</th>\n","      <th>Dur. (ms)</th>\n","      <th>IMSI</th>\n","      <th>MSISDN/Number</th>\n","      <th>IMEI</th>\n","      <th>Last Location Name</th>\n","      <th>...</th>\n","      <th>YouTube UL (Bytes)</th>\n","      <th>Netflix DL (Bytes)</th>\n","      <th>Netflix UL (Bytes)</th>\n","      <th>Gaming DL (Bytes)</th>\n","      <th>Gaming UL (Bytes)</th>\n","      <th>Other DL</th>\n","      <th>Other UL</th>\n","      <th>Total UL (Bytes)</th>\n","      <th>Total DL (Bytes)</th>\n","      <th>index</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.311448e+19</td>\n","      <td>4/4/2019 12:01</td>\n","      <td>770.0</td>\n","      <td>4/25/2019 14:35</td>\n","      <td>662.0</td>\n","      <td>1823652.0</td>\n","      <td>2.082014e+14</td>\n","      <td>3.366496e+10</td>\n","      <td>3.552121e+13</td>\n","      <td>9.16456699548519E+015</td>\n","      <td>...</td>\n","      <td>2501332.0</td>\n","      <td>8198936.0</td>\n","      <td>9656251.0</td>\n","      <td>278082303.0</td>\n","      <td>14344150.0</td>\n","      <td>171744450.0</td>\n","      <td>8814393.0</td>\n","      <td>36749741.0</td>\n","      <td>308879636.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.311448e+19</td>\n","      <td>4/9/2019 13:04</td>\n","      <td>235.0</td>\n","      <td>4/25/2019 8:15</td>\n","      <td>606.0</td>\n","      <td>1365104.0</td>\n","      <td>2.082019e+14</td>\n","      <td>3.368185e+10</td>\n","      <td>3.579401e+13</td>\n","      <td>L77566A</td>\n","      <td>...</td>\n","      <td>19111729.0</td>\n","      <td>18338413.0</td>\n","      <td>17227132.0</td>\n","      <td>608750074.0</td>\n","      <td>1170709.0</td>\n","      <td>526904238.0</td>\n","      <td>15055145.0</td>\n","      <td>53800391.0</td>\n","      <td>653384965.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.311448e+19</td>\n","      <td>4/9/2019 17:42</td>\n","      <td>1.0</td>\n","      <td>4/25/2019 11:58</td>\n","      <td>652.0</td>\n","      <td>1361762.0</td>\n","      <td>2.082003e+14</td>\n","      <td>3.376063e+10</td>\n","      <td>3.528151e+13</td>\n","      <td>D42335A</td>\n","      <td>...</td>\n","      <td>14699576.0</td>\n","      <td>17587794.0</td>\n","      <td>6163408.0</td>\n","      <td>229584621.0</td>\n","      <td>395630.0</td>\n","      <td>410692588.0</td>\n","      <td>4215763.0</td>\n","      <td>27883638.0</td>\n","      <td>279807335.0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.311448e+19</td>\n","      <td>4/10/2019 0:31</td>\n","      <td>486.0</td>\n","      <td>4/25/2019 7:36</td>\n","      <td>171.0</td>\n","      <td>1321509.0</td>\n","      <td>2.082014e+14</td>\n","      <td>3.375034e+10</td>\n","      <td>3.535661e+13</td>\n","      <td>T21824A</td>\n","      <td>...</td>\n","      <td>15146643.0</td>\n","      <td>13994646.0</td>\n","      <td>1097942.0</td>\n","      <td>799538153.0</td>\n","      <td>10849722.0</td>\n","      <td>749039933.0</td>\n","      <td>12797283.0</td>\n","      <td>43324218.0</td>\n","      <td>846028530.0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.311448e+19</td>\n","      <td>4/12/2019 20:10</td>\n","      <td>565.0</td>\n","      <td>4/25/2019 10:40</td>\n","      <td>954.0</td>\n","      <td>1089009.0</td>\n","      <td>2.082014e+14</td>\n","      <td>3.369980e+10</td>\n","      <td>3.540701e+13</td>\n","      <td>D88865A</td>\n","      <td>...</td>\n","      <td>18962873.0</td>\n","      <td>17124581.0</td>\n","      <td>415218.0</td>\n","      <td>527707248.0</td>\n","      <td>3529801.0</td>\n","      <td>550709500.0</td>\n","      <td>13910322.0</td>\n","      <td>38542814.0</td>\n","      <td>569138589.0</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 46 columns</p>\n","</div>"],"text/plain":["      bearer id            Start  Start ms              End  End ms  \\\n","0  1.311448e+19   4/4/2019 12:01     770.0  4/25/2019 14:35   662.0   \n","1  1.311448e+19   4/9/2019 13:04     235.0   4/25/2019 8:15   606.0   \n","2  1.311448e+19   4/9/2019 17:42       1.0  4/25/2019 11:58   652.0   \n","3  1.311448e+19   4/10/2019 0:31     486.0   4/25/2019 7:36   171.0   \n","4  1.311448e+19  4/12/2019 20:10     565.0  4/25/2019 10:40   954.0   \n","\n","   Dur. (ms)          IMSI  MSISDN/Number          IMEI  \\\n","0  1823652.0  2.082014e+14   3.366496e+10  3.552121e+13   \n","1  1365104.0  2.082019e+14   3.368185e+10  3.579401e+13   \n","2  1361762.0  2.082003e+14   3.376063e+10  3.528151e+13   \n","3  1321509.0  2.082014e+14   3.375034e+10  3.535661e+13   \n","4  1089009.0  2.082014e+14   3.369980e+10  3.540701e+13   \n","\n","      Last Location Name  ...  YouTube UL (Bytes)  Netflix DL (Bytes)  \\\n","0  9.16456699548519E+015  ...           2501332.0           8198936.0   \n","1                L77566A  ...          19111729.0          18338413.0   \n","2                D42335A  ...          14699576.0          17587794.0   \n","3                T21824A  ...          15146643.0          13994646.0   \n","4                D88865A  ...          18962873.0          17124581.0   \n","\n","   Netflix UL (Bytes)  Gaming DL (Bytes)  Gaming UL (Bytes)     Other DL  \\\n","0           9656251.0        278082303.0         14344150.0  171744450.0   \n","1          17227132.0        608750074.0          1170709.0  526904238.0   \n","2           6163408.0        229584621.0           395630.0  410692588.0   \n","3           1097942.0        799538153.0         10849722.0  749039933.0   \n","4            415218.0        527707248.0          3529801.0  550709500.0   \n","\n","     Other UL  Total UL (Bytes)  Total DL (Bytes)  index  \n","0   8814393.0        36749741.0       308879636.0      0  \n","1  15055145.0        53800391.0       653384965.0      1  \n","2   4215763.0        27883638.0       279807335.0      2  \n","3  12797283.0        43324218.0       846028530.0      3  \n","4  13910322.0        38542814.0       569138589.0      4  \n","\n","[5 rows x 46 columns]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# Function to calculate missing values by column\n","def missing_values_table(df):\n","    # Total missing values\n","    mis_val = df.isnull().sum()\n","\n","    # Percentage of missing values\n","    mis_val_percent = 100 * df.isnull().sum() / len(df)\n","\n","    # dtype of missing values\n","    mis_val_dtype = df.dtypes\n","\n","    # Make a table with the results\n","    mis_val_table = pd.concat([mis_val, mis_val_percent, mis_val_dtype], axis=1)\n","\n","    # Rename the columns\n","    mis_val_table_ren_columns = mis_val_table.rename(\n","    columns = {0 : 'Missing Values', 1 : '% of Total Values', 2: 'Dtype'})\n","\n","    # Sort the table by percentage of missing descending\n","    mis_val_table_ren_columns = mis_val_table_ren_columns[\n","        mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n","    '% of Total Values', ascending=False).round(1)\n","\n","    # Print some summary information\n","    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"\n","        \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n","          \" columns that have missing values.\")\n","\n","    # Return the dataframe with missing information\n","    return mis_val_table_ren_columns\n","\n","def format_float(value):\n","    return f'{value:,.2f}'\n","\n","def find_agg(df:pd.DataFrame, agg_column:str, agg_metric:str, col_name:str, top:int, order=False )->pd.DataFrame:\n","\n","    new_df = df.groupby(agg_column)[agg_column].agg(agg_metric).reset_index(name=col_name).\\\n","                        sort_values(by=col_name, ascending=order)[:top]\n","\n","    return new_df\n","\n","def convert_bytes_to_megabytes(df, bytes_data):\n","    \"\"\"\n","        This function takes the dataframe and the column which has the bytes values\n","        returns the megabytesof that value\n","\n","        Args:\n","        -----\n","        df: dataframe\n","        bytes_data: column with bytes values\n","\n","        Returns:\n","        --------\n","        A series\n","    \"\"\"\n","\n","    megabyte = 1*10e+5\n","    df[bytes_data] = df[bytes_data] / megabyte\n","    return df[bytes_data]\n","\n","def fix_outlier(df, column):\n","    df[column] = np.where(df[column] > df[column].quantile(0.95), df[column].median(),df[column])\n","\n","    return df[column]\n","\n","\n","###################################PLOTTING FUNCTIONS###################################\n","\n","def plot_hist(df:pd.DataFrame, column:str, color:str)->None:\n","    # plt.figure(figsize=(15, 10))\n","    # fig, ax = plt.subplots(1, figsize=(12, 7))\n","    sns.displot(data=df, x=column, color=color, kde=True, height=7, aspect=2)\n","    plt.title(f'Distribution of {column}', size=20, fontweight='bold')\n","    plt.show()\n","\n","def plot_count(df:pd.DataFrame, column:str) -> None:\n","    plt.figure(figsize=(12, 7))\n","    sns.countplot(data=df, x=column)\n","    plt.title(f'Distribution of {column}', size=20, fontweight='bold')\n","    plt.show()\n","\n","def plot_bar(df:pd.DataFrame, x_col:str, y_col:str, title:str, xlabel:str, ylabel:str)->None:\n","    plt.figure(figsize=(12, 7))\n","    sns.barplot(data = df, x=x_col, y=y_col)\n","    plt.title(title, size=20)\n","    plt.xticks(rotation=75, fontsize=14)\n","    plt.yticks( fontsize=14)\n","    plt.xlabel(xlabel, fontsize=16)\n","    plt.ylabel(ylabel, fontsize=16)\n","    plt.show()\n","\n","def plot_heatmap(df:pd.DataFrame, title:str, cbar=False)->None:\n","    plt.figure(figsize=(12, 7))\n","    sns.heatmap(df, annot=True, cmap='viridis', vmin=0, vmax=1, fmt='.2f', linewidths=.7, cbar=cbar )\n","    plt.title(title, size=18, fontweight='bold')\n","    plt.show()\n","\n","def plot_box(df:pd.DataFrame, x_col:str, title:str) -> None:\n","    plt.figure(figsize=(12, 7))\n","    sns.boxplot(data = df, x=x_col)\n","    plt.title(title, size=20)\n","    plt.xticks(rotation=75, fontsize=14)\n","    plt.show()\n","\n","def plot_box_multi(df:pd.DataFrame, x_col:str, y_col:str, title:str) -> None:\n","    plt.figure(figsize=(12, 7))\n","    sns.boxplot(data = df, x=x_col, y=y_col)\n","    plt.title(title, size=20)\n","    plt.xticks(rotation=75, fontsize=14)\n","    plt.yticks( fontsize=14)\n","    plt.show()\n","\n","def plot_scatter(df: pd.DataFrame, x_col: str, y_col: str, title: str, hue: str, style: str) -> None:\n","    plt.figure(figsize=(12, 7))\n","    sns.scatterplot(data = df, x=x_col, y=y_col, hue=hue, style=style)\n","    plt.title(title, size=20)\n","    plt.xticks(fontsize=14)\n","    plt.yticks( fontsize=14)\n","    plt.show()\n","\n","\n","pd.options.display.float_format = format_float"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"DataModelling.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
